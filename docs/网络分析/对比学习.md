关键词：对比学习（Contrastive Learning）、图神经网络（Graph Neural Networks，GNNs）、自监督学习（self-supervised learning，SSL）、预训练（pre-training）、互信息（Mutual Information，MI）

文献列表：

1. Graph Contrastive Learning with Augmentations
2. Contrastive Multi-View Representation Learning on Graphs
3. Graph Representation Learning via Graphical Mutual Information Maximization



## 理论学习

### 技术博客

| 标题                                                         | 链接                                                        | 笔记                                                         |
| ------------------------------------------------------------ | ----------------------------------------------------------- | ------------------------------------------------------------ |
| 图神经网络+对比学习，下一步去哪？                            | https://zhuanlan.zhihu.com/p/427395156                      | 大概读了一下，了解图对比学习的顶层设计。                     |
| 信息论（1）——熵、互信息、相对熵                              | https://zhuanlan.zhihu.com/p/36192699                       | 熵是衡量随机变量不确定性的度量，也表示着信息量的多少，写作数学公式为$H(X)=-\sum_{x\in\mathcal{X}}{p(x)\log{p(x)}}$。互信息$I(X;Y)=H(X)-H(X|Y)$表示知道事实Y后，原来的信息量（不确定性）减少了多少。 |
| 论文详解：Contrastive Multi-View Representation Learning on Graphs | https://blog.csdn.net/Patricia886/article/details/114922324 | 基础概念扫盲，图示很生动，简要的介绍了MVGRL。                |
| 互信息及其在图表示学习中的应用                               | https://zhuanlan.zhihu.com/p/149743192                      | MINE利用神经网络估计互信息的范式。互信息是联合分布和边缘分布的散度距离。 |
| 学习篇—顶会Paper复现方法                                     | https://zhuanlan.zhihu.com/p/389294219                      |                                                              |
| 图自监督学习和预训练论文合集                                 | https://zhuanlan.zhihu.com/p/427218683                      |                                                              |
| Mutual Information Neural Estimator(MINE)：通过样本有效估计高维连续数据互信息 | https://zhuanlan.zhihu.com/p/412538959                      | 用样本估计分布难度难度太大，输入数据太稀疏；MINE用神经网络分析神经网络，通过寻找最优的函数计算期望的差值，而不必知道具体的分布，抬高互信息的下界。 |
| 如何衡量两组向量的相似度，有什么好的方法？                   | https://www.zhihu.com/question/272195313/answer/2260755496  | 余弦 点积 相似度                                             |
| 论文阅读｜浅谈图上的自监督学习——对比学习                     | https://zhuanlan.zhihu.com/p/187247235                      |                                                              |
| 【ICML 2020 图上的自监督对比学习】Contrastive Multi-View Representation Learning on Graphs | https://zhuanlan.zhihu.com/p/388280298                      |                                                              |
| 自监督对比学习（Contrastive Learning）综述+代码              | https://zhuanlan.zhihu.com/p/334732028                      |                                                              |
| 数据预处理：数据归一化MinMaxScaler                           | https://blog.51cto.com/u_15072918/2580389                   | 无量纲化                                                     |
| 使用Sklearn的MinMaxScaler做最简单的归一化                    | https://www.jianshu.com/p/9eb87ca8a921                      | 数据标准化（normalization）包括数据同趋化处理和无量纲化处理  |
| 无监督Graph Embedding神器DGI，你学会了吗？                   | https://zhuanlan.zhihu.com/p/396341298                      | ![无监督Graph Embedding神器DGI，你学会了吗？](https://pic3.zhimg.com/v2-83c2ce5191ab2b1896e340a268a11b7c_1440w.jpg?source=172ae18b) |
| 论文分享：Graph Contrastive Learning with Augmentations      | https://zhuanlan.zhihu.com/p/460008173                      |                                                              |

### GraphCL

文章的贡献：

1. **设计**了四种图增强方法
2. 实验检测了**不同组合的**图增强方法的效果
3. 初步实验了**参数化**的图增强方法

代码：https://github.com/Shen-Lab/GraphCL

GNN缺少预训练，因为为避免“过平滑”或“信息丢失”，通常会采用浅层模型，或采用正则化器（regularizer）来训练深层模型。

GNN需要预训练的原因：数据规模在变大，更好的为浅层模型初始化参数。

图数据中往往具有丰富的上下文结构化信息，很难为下流任务设计通用的GNN pre-training模式。

四种**图增强方法（Data Augmentation for Graphs）**：Node dropping、Edge perturbation、Attribute masking、Subgraph。

- Node dropping：随机丢弃部分节点和其连边，假设丢失部分节点不影响图的语义。
- Edge perturbation：按一定比例随机增加或丢弃边，假设图的语义对边的连接性不同有一定的鲁棒性。
- Attribute masking：用上下文信息恢复遮盖的节点属性，假设丢失部分节点属性不影响模型预测。
- Subgraph：随机游走采样子图，假设局部结构中含有图的语义。

**最大化同一幅图的两个增强视图（augmented view）的一致性**，通过潜在空间的对比损失。

增强视图（Augmented View）的选择往往与数据或任务相关。



### MVGRL

将图的增强方式分为两类：

1. feature-space：在节点特征上操作，如masking或加Gaussian噪声。
2. structure-space：变换图结构，如加/去边，子采样，生成全局视图（最短路/diffusion matrices）

### GMI

Deep InfoMax method:maximize MI between its inputs and outputs 视觉领域

Deep Graph Infomax(DGI):

GMI规避了readout不是injective的风险，直接在输入图与高级嵌入间计算MI。

## 问题

1. 什么是对比学习（Contrastive Learning）？

无监督学习，利用未标记的数据，发现数据中的潜在模式/结构。

进一步可分为：

- 生成式学习：以自编码器（GAN、VAE etc.）为代表，由数据生成数据。
- 对比式学习：学习同类实例之间的共同特征，区分非同类实例之间的不同之处。

与生成式学习比较，对比式学习不需要关注实例上繁琐的细节，只需要**在抽象语义级别的特征空间**上学会对数据的区分即可，因此模型以及其优化变得更加简单，且泛化能力更强。

**对比学习**的目标是学习一个编码器，此编码器**对同类数据进行相似的编码，并使不同类的数据的编码结果尽可能的不同**。
