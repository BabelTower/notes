# A Tutorial on Network Embeddings

Network Embeddings的**本质**是一个将网络节点映射为低维向量的函数。

## 定义
- heterogeneous network 节点和边类型不止一种。
- signed graph 边带权重。

## DeepWalk
**作用：** 在 network embeddings 和 word embeddings 架起了桥梁。

**方法：** 将点视作单词，生成 short random walks 作为句子。然后使用 neural language model（如 Skip-gram ）来获取 network embedding 。

**受欢迎的原因：** 
- 在线算法
- 可并行化
- 引入了范式
- 可扩展性好（图的复杂性，范式中第2步和第3步所能采用的策略）

**范式的步骤：**
1. 选择一个与图相关的矩阵（ random walk transition matrix / normalized Laplacian matrix / the  powers of theadjacency matrix）
2. 图采样生成节点序列（可选的步骤）
3. 从矩阵/生成序列中学习node embeddings (DeepWalk采用了Skip-gram)

## unsupervise +  undirected

语言建模和网络建模内在存在相似性，转化成nlp问题，用nlp的method解决。

Skip-gram预测临近单词，即上下文单词出现的条件概率。
分为两阶段：
1. 识别出上下文的单词
2. 最大化条件概率

DeepWalk
1. 识别上下文节点，生成随机游走
2. 学习嵌入，最大化预测上下文的可能性

上下文节点的来源和嵌入学习算法，导致了无监督表示学习的不同算法。

！超参非常重要，控制图中每个节点的上下文节点的分布。

## 问题
1. DeepWalk的算法实现中为什么要建立一棵二叉树？