---
Title: 25-参数设置
Date: 2021-12-05
---

## keywords
#调参 
## 笔记
![](assets/Pasted%20image%2020211205231124.png)

### 参数设置

![](assets/Pasted%20image%2020211205232011.png)

1. SGD：每个batch用梯度的平均值更新参数
	![](assets/Pasted%20image%2020211205232215.png)

2. 激活函数：
	![](assets/Pasted%20image%2020211205232812.png)
	
3. 训练数据的初始化：归一化

	$newX = \frac{X-mean(X)}{std(X)}$

4. 梯度消失：初始化$(W,b)$要在零附近，从区间$(-\frac{1}{\sqrt{d}}, \frac{1}{\sqrt{d}})$ 均匀随机取值。
	![](assets/Pasted%20image%2020211205233600.png)
	
	如果参数全都在0处，模型会非常接近线性模型。
	
5. Batch Normalization ：直接把每一层的值均值和方差归一化 

	最后不加放缩的话，BN层的效果非常接近线性模型，效果不好。
	
	BN在SGD中的实现？前向和后向。
	
6. 目标函数选择：
	1. 正则项
	2. 分类问题：最后一层softmax映射为和为1的概率， 目标函数采用交叉熵。（之前的课程里用的是MeanSquareError）

7. 参数更新的策略
	1. 常规的更新学习率固定，几何上会折来折的去走，优化的路径不是最好，梯度方向不一致 （值）
		1. AdaGrad 会叠加之前的梯度
		2. RMSProp 叠加之前的梯度加了个超参
	2. SGD求梯度策略过于随机，解决梯度随机性的问题：（方向）
		1. SGD+Momentum路线会平滑一些
	3. Adam综合解决了以上两个问题 $\rho_1=0.9, \rho_2=0.999$ 

![](assets/Pasted%20image%2020211206002326.png)

## 总结
