

熵是一个随机变量不确定性的度量。

离散型随机变量 $X\sim p(x)$，其离散熵可以定义为

$$H(X)=-\sum_{x\in \chi}p(x)\log p(x)$$

熵越大 = 不确定性越大 = 信息量越大

熵 = 平均意义上对随机变量的编码长度 = 随机变量X的函数$log\frac{1}{p(x)}$的期望 = 信息量

必然事件的熵 = 0

互信息 不确定性变化  知道了事实Y后，原来的信息量减少了多少。

$$ I(X;Y)=H(X)-H(X|Y) $$

直觉上，如果X和Y相关，如果我们知道了Y，理应可以知道一些X的信息。

相对熵：两个随机变量的概率分布之间的差异程度量化

$$D(p\Vert q)=\sum_{x\in \chi}p(x)\log \frac{p(x)}{q(x)}$$

互信息衡量相关性，是相对熵的期望。

NMI标准化互信息，因为熵会随着簇的数目的增长而增大，用熵做分母将MI值调整到0与1之间。


信息论（1）——熵、互信息、相对熵 - 徐光宁的文章 - 知乎 https://zhuanlan.zhihu.com/p/36192699