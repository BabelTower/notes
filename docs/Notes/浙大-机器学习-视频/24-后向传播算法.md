Title: 24-后向传播算法
Date: 2021-12-03
## keywords

#BP算法 #微积分 #泰勒展开

## 笔记

### 前言
数据 -> 模型参数
如何选择合理的神经网络？宽度、深度的选择？目前理论不完备，考实验、经验。

### BP算法
Back Propogation 后向传播算法
主要思想：梯度下降法（Gradient Descent Method）求局部最大值。

一阶泰勒展开公式：
$$
f(\omega+\Delta \omega) = f(\omega) + \frac{df(\omega)}{d\omega} |_{\omega} * \Delta \omega + o(\Delta \omega)
$$

BP算法更新参数的公式：

$$
\omega_{k+1} = \omega_{k} - \alpha * \frac{df(\omega)}{d\omega} |_{\omega_k}
$$

证明每次更新参数后，目标函数都会比之间更小：

$$
\begin{aligned}
f(\omega_{k+1}) & = f(\omega_k) + (\frac{df(\omega)}{d\omega} |_{\omega_k})(-\alpha\frac{df(\omega)}{d\omega} |_{\omega_k}) + o(\Delta \omega)  \\
& = f(\omega_k) -\alpha(\frac{df(\omega)}{d\omega} |_{\omega_k})^2 + o(\Delta \omega) < f(\omega_k) 
\end{aligned}
$$

首先前向计算模型当前输出，用链式求导法则来求目标函数对参数的导数（自动微分），然后用梯度下降更新模型参数。

### 阶跃函数
1. sigmoid:
$$\varphi(x)=\frac{1}{1+e^{-x}}$$
$$\varphi^{\prime}(x)=\varphi(x)*[1-\varphi(x)]$$

2. tanh:
$$\varphi(x)=\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$$
$$\varphi^{\prime}(x)=1-[\varphi(x)]^2$$

3. ReLU(Rectified Linear Units):
$$ \varphi(x) = max(0, x) $$
$$ \varphi^{\prime}(x) = 
\begin{cases}
1, & x > 0 \\
0, & x \le 0
\end{cases}
$$
ReLU：神经网络每一层的活跃度降低。
4. Leak ReLU:
$$ \varphi(x) = 
\begin{cases}
x, & x > 0 \\
\beta x, & x \le 0
\end{cases}
$$
$$ \varphi^{\prime}(x) = 
\begin{cases}
1, & x > 0 \\
\beta, & x \le 0
\end{cases}
$$


### 标准形式

第1层：
$$x=a^{(0)}$$
$$z^{(1)}=W^{(1)}a^{(0)}+b^{(1)}$$
$$a^{(1)} = \varphi(z^{(1)})$$

第2层：
$$z^{(2)}=W^{(2)}a^{(1)}+b^{(2)}$$
$$a^{(2)} = \varphi(z^{(2)})$$

第3层：
$$z^{(3)}=W^{(3)}a^{(2)}+b^{(3)}$$
$$a^{(3)} = \varphi(z^{(3)})$$

$\ldots$

第L层：
$$z^{(l)}=W^{(l)}a^{(l-1)}+b^{(l)}$$
$$a^{(l)} = \varphi(z^{(l)})$$

**定义：**
![](assets/Pasted%20image%2020211203131904.png)

Minimize:$E=\frac{1}{2}\Vert y-Y\Vert^2$

设$\delta^{(m)}_i=\frac{\partial E}{\partial Z^{(m)}_i}$，则有：
$$
\delta^{(l)}_i=\frac{\partial E}{\partial Z^{(l)}_i}=\frac{\partial E}{\partial y_i}\frac{\partial y_i}{\partial Z^{(l)}_i}
=(y_i-Y_i)\varphi^{\prime}(Z^{(l)}_i)
$$

下面是$\delta^{(m)}_i$与$\delta^{(m+1)}_i$的关系：
$$\delta^{(m)}_i=[\sum_{j=1}^{S_{m+1}}\delta^{(m+1)}_iW_{ji}^{(m+1)}]\varphi^{\prime}(Z^{(m)}_i)$$

![](assets/Pasted%20image%2020211203135336.png)

## 总结